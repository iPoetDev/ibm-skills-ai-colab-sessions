{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iPoetDev/ibm-skills-ai-colab-sessions/blob/main/Session3_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Avx6qaZ_jrV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the MNIST dataset"
      ],
      "metadata": {
        "id": "Ptv08Kdvfn9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load MNIST dataset - pictures of handwritten numbers\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Convert the Data to Floats and Normalize\n",
        "# data is made with numbers between 0 and 255\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# The pictures are 28x28 pixels, each pixel has a number that shows how dark it is\n",
        "# We reshape the pictures to tell the computer that each picture is 28x28 and\n",
        "# 1 color channel because they are black and white pictures\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))"
      ],
      "metadata": {
        "id": "BEo38NyVOfV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "aZZPo9awf2RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "latent_dim = 2\n",
        "# Setting up the input encoder, the shape must match our data\n",
        "encoder_inputs = tf.keras.Input(shape=(28, 28, 1))\n",
        "\n",
        "# First convolutional layer - applies filter to the input  image to highlight\n",
        "# important features\n",
        "# We are use 32 filters, each filter is 3x3 pixels, use relu activation\n",
        "# Strides means move the filter 2 pixels at time\n",
        "# Padding same mean keep output size same as input size\n",
        "x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)\n",
        "\n",
        "# Similar to First convolutional layer but with 64 filters, learns more complex\n",
        "# featurs from the images\n",
        "x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "\n",
        "# Flattens to 1D\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation='relu')(x)\n",
        "\n",
        "# Represents the latent space, basically summarizing in just a few key points\n",
        "z_mean = layers.Dense(latent_dim)(x)\n",
        "z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "MAX99DqeOj_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE Sampling"
      ],
      "metadata": {
        "id": "zLH8KVSff8pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling function for the VAE\n",
        "# This function is generating new samples in the latent space by adding\n",
        "# some random noise to the simplified data representation. This is important for\n",
        "# creating diverse and realistice outputs in the models like a VAE.\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# the function we just defined using a sampling function 'Lambda'\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "\n"
      ],
      "metadata": {
        "id": "WIVY3QfhOqXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "9nrEB_ITgC0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "# This code is building the decoder part of a Variational Autoencoder (VAE).\n",
        "# The decoder takes the simplified latent representation (latent_dim) and\n",
        "# transforms it back into the original image format through a series of layers.\n",
        "# These layers gradually upscale and reshape the data until it matches the\n",
        "# size of the original input images, effectively reconstructing the images\n",
        "# from the compressed latent space.\n",
        "decoder_inputs = tf.keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation='relu')(decoder_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "CeX-PAiYOw1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE Model"
      ],
      "metadata": {
        "id": "rjShiZR3gGLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE Model\n",
        "# This code is creating a Variational Autoencoder (VAE) by combining the\n",
        "# encoder and decoder models. The encoder compresses the input images into\n",
        "# a simplified latent representation, and the decoder reconstructs the images\n",
        "# from this latent space. The final VAE model takes input images,\n",
        "# processes them through the encoder to get the latent representation,\n",
        "# and then uses the decoder to output the reconstructed images.\n",
        "encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "decoder = tf.keras.Model(decoder_inputs, decoder_outputs, name='decoder')\n",
        "outputs = decoder(encoder(encoder_inputs)[2])\n",
        "vae = tf.keras.Model(encoder_inputs, outputs, name='vae')"
      ],
      "metadata": {
        "id": "pyZAhtjVO4eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE Loss"
      ],
      "metadata": {
        "id": "iZPZR3HngJp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE loss\n",
        "# This code block is defining and adding a custom loss function to\n",
        "# the Variational Autoencoder (VAE). It combines the reconstruction loss,\n",
        "# which measures how well the VAE reconstructs the input images, and the\n",
        "# KL divergence loss, which ensures the latent space is well-behaved and\n",
        "# regularized. The combined loss helps the VAE learn to generate realistic\n",
        "# and diverse outputs. Finally, the VAE is compiled with\n",
        "# the Adam optimizer for training.\n",
        "reconstruction_loss = tf.keras.losses.binary_crossentropy(tf.keras.backend.flatten(encoder_inputs), tf.keras.backend.flatten(outputs))\n",
        "reconstruction_loss *= 28 * 28\n",
        "kl_loss = 1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var)\n",
        "kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "\n"
      ],
      "metadata": {
        "id": "VInaOSS7O8P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "Y4SeQikcgM8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "vae.fit(x_train, epochs=30, batch_size=128, validation_data=(x_test, None))\n",
        "\n"
      ],
      "metadata": {
        "id": "pAWZdwlkPA4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display the generated images - function"
      ],
      "metadata": {
        "id": "zho9LQCegQe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display generated images\n",
        "def plot_latent_space(decoder, n=30, figsize=15):\n",
        "    # Display a n*n 2D manifold of digits\n",
        "    digit_size = 28\n",
        "    scale = 1.0\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # Linearly spaced coordinates on the unit square were transformed\n",
        "    # through the inverse CDF (ppf) of the Gaussian to produce values\n",
        "    # of the latent variables z, since the prior of the latent space\n",
        "    # is Gaussian\n",
        "    grid_x = np.linspace(-scale, scale, n)\n",
        "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap='Greys_r')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "i_UzwteuPF6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Model\n"
      ],
      "metadata": {
        "id": "RFOnrVypgWu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run the model\n",
        "plot_latent_space(decoder)\n"
      ],
      "metadata": {
        "id": "_8idKqGJPLVp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}