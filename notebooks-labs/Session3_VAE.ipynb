{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/iPoetDev/ibm-skills-ai-colab-sessions/blob/main/notebooks-labs/Session3_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCxGlesu71Qx"
   },
   "source": [
    "# <ins>Session 3</ins>.1: **IBM Skills Build: Generative AI Live Technical Lab** (Part 1)\n",
    "\n",
    "> #### **Objective**: *Understand the theory and hands-on implementation of*: <br>  1️⃣ Variational AutoEncoders(VAE)\n",
    ">> - Exploring AutoEncoders 4 layers\n",
    ">> - Displaying generated images, like handwriting\n",
    "\n",
    "- **URL**: [https://skills.yourlearning.ibm.com/activity/PLAN-CB1CC0D21AFB](https://skills.yourlearning.ibm.com/activity/PLAN-CB1CC0D21AFB \"Programme for Artifical Intelligence: eLearning on IBM.com (Login required)\") &nbsp;<small><sup><strong> eLearning, Login</strong></sup></small><br>\n",
    "- **Share**: [Introduction to Generative AI](https://skills.yourlearning.ibm.com/activity/MDL-388 \"eLearning on IBM.com (Login required\") &nbsp;<small><sup><strong>eLearning, Login</strong></sup></small>\n",
    "- **Recording**: [Recording: Live Technical Session 3](https://skills.yourlearning.ibm.com/activity/URL-6BF19B3CC379 \"Video: IBM's Box.com (Login required\")\n",
    "- **CoLab: Source Notebook**: [https://colab.research.google.com/drive/1eD7pRKmhVFl0nfwzsoIy9RTtoPMVkZPW?usp=sharing](https://colab.research.google.com/drive/1eD7pRKmhVFl0nfwzsoIy9RTtoPMVkZPW?usp=sharing \"Authors: Marty Bradly's Session 3 VAE notebook\")\n",
    "  - Original by author: Marty Bradly: [LinkedIn](https://www.linkedin.com/in/martybradley/), [Website](https://www.evergreen-ai.com/), [GitHub @marty916](https://github.com/marty916 \"Marty Bradly [July, 2024], Last accessed: July 2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEKLVi77ZvZX"
   },
   "source": [
    "<small>Notebook for technical audiences | See README and Sessions.md for business and product audiences</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKKynZLwfHTn"
   },
   "source": [
    "> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al8RNEfh9T_h"
   },
   "source": [
    "## GitHub\n",
    "\n",
    "- **IBM-Skills-AI_Colab-Sessions**:\n",
    "    - [README](https://github.com/iPoetDev/ibm-skills-ai-colab-sessions/blob/main/README.md)\n",
    "    - [Sessions Summary](https://github.com/iPoetDev/ibm-skills-ai-colab-sessions/blob/main/Sessions.md)\n",
    "    - [notebook-labs/Session3_VAE.ipynb](https://github.com/iPoetDev/ibm-skills-ai-colab-sessions/blob/main/notebooks-labs/Session3_VAE.ipynb \"@iPoetDev: GitHub.com:  IBM-Skills-AI_Colab-Sessions: Session3_VAE Juypter Notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIj5Uzfv--vm"
   },
   "source": [
    "## Steps\n",
    "\n",
    "1.   [Setup/Imports](#scrollTo=UGD_IeVp_zpv&line=1&uniqifier=1)\n",
    "2.   [Load dataset](#scrollTo=Ptv08Kdvfn9I&line=1&uniqifier=1)\n",
    "3.   [Encoder](#scrollTo=aZZPo9awf2RE&line=1&uniqifier=1)\n",
    "4.   [VAE Sampling](#scrollTo=zLH8KVSff8pH&line=1&uniqifier=1)\n",
    "5.   [Decoder](#scrollTo=9nrEB_ITgC0I&line=1&uniqifier=1)\n",
    "6.   [VAE Model](#scrollTo=rjShiZR3gGLq&line=1&uniqifier=1)\n",
    "7.   [VAE Loss](#scrollTo=iZPZR3HngJp8&line=1&uniqifier=1)\n",
    "8.   [Model Training](#scrollTo=Y4SeQikcgM8i&line=1&uniqifier=1)\n",
    "9.   [Display Function: Plot](#scrollTo=zho9LQCegQe0&line=1&uniqifier=1)\n",
    "10.  [Model Execution](#scrollTo=RFOnrVypgWu6&line=2&uniqifier=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPYImMPOe_Or"
   },
   "source": [
    "---\n",
    "> <hr>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGD_IeVp_zpv"
   },
   "source": [
    "## 1. <ins> Setup / Imports<ins>\n",
    "\n",
    "*   TensorFlow\n",
    "    - Shaping in Sampling\n",
    "    - Keras (below)\n",
    "*   Keras.io / TensorFlow Keras:\n",
    "    - Datasets : MINST Data\n",
    "    - Inputs: Encoders, Decoder's Latent representaton inputs\n",
    "    - Models: VAE Model (Encoder, Decoder)\n",
    "    - Losses: Binary (Cross Entropy)\n",
    "    - Backend:\n",
    "        - Random Noise to Sampling,\n",
    "        - Calculations for VAE losses.\n",
    "*   NumPy: Contain/Manipluate Data units.\n",
    "*   MatplotLib: Ploting / generating graphics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Avx6qaZ_jrV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icdVVGzQac4V"
   },
   "source": [
    "> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ptv08Kdvfn9I"
   },
   "source": [
    "## 2. <ins>Load the MNIST dataset</ins>\n",
    "\n",
    "> Load MNIST dataset - pictures of handwritten numbers\n",
    "\n",
    "1. Load Dataset\n",
    "2. Conversion: Floats, Normalise per Train/Test sets\n",
    "3. Reshape the pictures\n",
    "   - By transforms the origina; data set array into a 4D array where each element represents a single grayscale image of size 28x28 pixels.\n",
    "   -  This format is commonly used as input for CNNs\n",
    "\n",
    "- Consants for cleaner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBpqbeBiPw7y"
   },
   "outputs": [],
   "source": [
    "DATA_NORMALISE = 255\n",
    "DATA_FLOAT = 'float32'\n",
    "DATA_WIDTH = 28\n",
    "DATA_HEIGHT = 28\n",
    "DATA_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEo38NyVOfV4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load MNIST dataset - pictures of handwritten numbers\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Convert the Data to\n",
    "# i) Floats\n",
    "# ii) Normalize\n",
    "# AS data is made with numbers between 0 and 255\n",
    "\n",
    "# A) Training Set\n",
    "x_train = x_train.astype(DATA_FLOAT) / DATA_NORMALISE.\n",
    "\n",
    "# B) Test Set\n",
    "x_test = x_test.astype(DATA_FLOAT) / DATA_NORMALISE.\n",
    "\n",
    "## ===\n",
    "\n",
    "# Reshapping: the pictures for computer so that each picture is\n",
    "# - 28x28\n",
    "# - 1 color channel\n",
    "# Pictures: 28x28 pixels, each pixel assigned a number to shows how dark it is.\n",
    "# - as pictures because they are black and white pictures (1 channel)\n",
    "\n",
    "# Reshape: Train Set\n",
    "\n",
    "# np.reshape: changes array shape\n",
    "x_train = np.reshape(x_train, (len(x_train),  # len() Nos of units in set\n",
    "                               DATA_WIDTH,    # width of each unit\n",
    "                               DATA_HEIGHT,   # height of each unit\n",
    "                               DATA_CHANNELS))\n",
    "                                    # number of channels, greyscale\n",
    "\n",
    "# Reshape: Test Set\n",
    "x_test = np.reshape(x_test, (len(x_test),\n",
    "                               DATA_WIDTH,\n",
    "                               DATA_HEIGHT,\n",
    "                               DATA_CHANNELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JdXYl03aegO"
   },
   "source": [
    "> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZZPo9awf2RE"
   },
   "source": [
    "## 3. <ins>Encoder</ins>\n",
    "\n",
    "1.   Set Encoder's Latent Dim\n",
    "2.   Setup Encoder's Input Shape\n",
    "3.   Define 1st Convoluntional Layer\n",
    "     - Applies filter to the input image\n",
    "       - Used to highlight important features\n",
    "       - Use 32 filters\n",
    "       - Per filter size: 3x3 pixels\n",
    "     - Use `relu` activation\n",
    "       - choice of activation function depends on the specific task\n",
    "       - the desired properties of the network.\n",
    "     - Use Strides to +2px/per move\n",
    "     - Padding: Output size === Input size\n",
    "4.  Flatten to Single Dimension\n",
    "    - the output of convolutional layers (CNN) is typically a multi-dimensional tensor.\n",
    "       -  a multi-dimensional tensor - represent feature maps.\n",
    "    - transforms this multi-dimensional tensor into a 1D vector.\n",
    "5.  Neuornal Density\n",
    "    - Is a fully connected (dense) layer\n",
    "    - Each neuron in this layer is connected to every neuron in the previous layer.\n",
    "    - Performs a linear transformation on the flattened input x\n",
    "    - Applies an activation function\n",
    "\n",
    "\n",
    "- Why Flatten? Flattening is often necessary before connecting to a fully connected (dense) layer, as dense layers expect their input to be a 1D vector\n",
    "- Combining Flattening with neuronal density, is common in  neural networks, especially after convolutional layers, to enable learning complex patterns and relationships in the data.\n",
    "\n",
    "\n",
    "6.  Latent Space Characteristics\n",
    "    - Crucial components of VAE architecture<br>\n",
    "    a. Mean of Latent Space<br>\n",
    "    b. Log-Variance of Latent Space\n",
    "    -  A lower-dimensional representation of the input data, capturing its essential features.\n",
    "    - Mean and Variance: The mean and variance of the latent space distribution control the location and spread of the encoded data points in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ujO9xHqa-dN"
   },
   "source": [
    "- Constants for clear code and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1h2QFzLTdA0"
   },
   "outputs": [],
   "source": [
    "LATENT_DIM_SIZE = 2     # Dimensionality: Latent Space: Complexity of represent.\n",
    "DATA_SHAPE = (DATA_WIDTH, DATA_HEIGHT, DATA_CHANNELS)\n",
    "FILTER_SIZE = 3         # 3x3 Filter size\n",
    "ACTIVATION_FUNCTION = 'relu' #  'relu', 'sigmoid', and 'tanh'\n",
    "STRIDE = 2              # Nos of Pixels to move by\n",
    "PADS = 'same'           # Padding: Output size === Input size\n",
    "FEATURES_SIMPLE = 32    # 32 Filters for Important Features\n",
    "FILTERS_COMPLEX = 64    # 64 Filters for more Complex Features\n",
    "LAYER_NEURONS = 16      # Specifies nos of neurons (units) in the dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAX99DqeOj_p"
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "latent_dim = LATENT_DIM_SIZE\n",
    "\n",
    "# Setting up the input encoder, the shape must match our data\n",
    "encoder_inputs = tf.keras.Input(shape=DATA_SHAPE)\n",
    "\n",
    "## 2 Dimensions+\n",
    "\n",
    "# First convolutional layer: Simple/Important Features (32)\n",
    "x = layers.Conv2D(FEATURES_SIMPLE,\n",
    "                  FILTER_SIZE,\n",
    "                  activation=ACTIVATION_FUNCTION,\n",
    "                  strides=STRIDE,\n",
    "                  padding=PADS)(encoder_inputs)\n",
    "\n",
    "# Similar Convolutional layer: Complex Features (64)\n",
    "x = layers.Conv2D(FEATURES_COMPLEX,\n",
    "                  FILTER_SIZE,\n",
    "                  activation=ACTIVATION_FUNCTION,\n",
    "                  strides=STRIDE,\n",
    "                  padding=PADS)(x)\n",
    "\n",
    "## 1 Dimensions\n",
    "\n",
    "# Flattens to 1D\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(LAYER_NEURONS,\n",
    "                 activation=ACTIVATION_FUNCTION,)(x)\n",
    "\n",
    "## Latent Space Characteristics\n",
    "# Represents the latent space, basically summarizing in just a few key points\n",
    "\n",
    "# creates a dense layers maping the input to a vector:\n",
    "# 1) mean of the latent space distribution.\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "# 2) logarithm of the variance of the latent space distribution.\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gomWm5C8agVZ"
   },
   "source": [
    "> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLH8KVSff8pH"
   },
   "source": [
    "## 4. <ins>VAE Sampling</ins>\n",
    "\n",
    "> This function is generating new samples in the latent space by adding\n",
    "some random noise to the simplified data representation. <br>\n",
    "> This is important for creating diverse and realistice outputs in the models like a VAE.\n",
    "\n",
    "1.   Sampling Function\n",
    "     - Batching\n",
    "     - Dimensionality\n",
    "     - Randon Noise\n",
    "     - Return New Sample\n",
    "2.   Layers Lambda sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThBzhhmddRgb"
   },
   "outputs": [],
   "source": [
    "# VAE Sampling Constants\n",
    "\n",
    "BATCH_SCOPE = 0\n",
    "DIM_SCOPE = 1\n",
    "EXP_SCOPE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIVY3QfhOqXv"
   },
   "outputs": [],
   "source": [
    "# Sampling function for the VAE\n",
    "\n",
    "def sampling(args):\n",
    "    # Split args\n",
    "    z_mean, z_log_var = args\n",
    "    # Sampling: Batch\n",
    "    batch = tf.shape(z_mean)[BATCH_SCOPE]\n",
    "    # Sampling: Dimension\n",
    "    dim = tf.shape(z_mean)[DIM_SCOPE]\n",
    "    # Sampling: Epsilon | Random Noise\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    # Sampling: Return New Sample\n",
    "    return z_mean + tf.keras.backend.exp(EXP_SCOPE * z_log_var) * epsilon\n",
    "\n",
    "# the function we just defined using a sampling function 'Lambda'\n",
    "z = layers.Lambda(sampling)([z_mean,\n",
    "                            z_log_var])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPyVHbRUaiCb"
   },
   "source": [
    "> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nrEB_ITgC0I"
   },
   "source": [
    "## 5. <ins>Decoder</ins>\n",
    "\n",
    "> - This code is building the decoder part of a Variational Autoencoder (VAE).\n",
    "  - The decoder takes the simplified latent representation (latent_dim) and transforms it back into the original image format through a series of layers.\n",
    "> - These layers gradually upscale and reshape the data until it matches the size of the original input images, effectively reconstructing the images from the compressed latent space.\n",
    "\n",
    "1.   Set Latent Dimensionality\n",
    "2.   Decoder Inputs define\n",
    "3.   Define Layer Density\n",
    "4.   Reshape Layers\n",
    "5.   Conv2DTranspose Layers:\n",
    "     - Upsample and extract features\n",
    "     - i. Most features\n",
    "     - ii. Highlighted features\n",
    "6.   Output Layer:\n",
    "     - Generate the final reconstructed image:\n",
    "     - Sigmoid activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cf3B1esG_G-"
   },
   "outputs": [],
   "source": [
    "LATENT_DIM_SIZE = 2      # Dimensionality: Latent Space: Complexity of represent\n",
    "SPATIAL_ALPHA = 7        # First Spatial Dimensions\n",
    "SPATIAL_OMEGA = 7        # Second Spatial Dimensions\n",
    "DECODE_KERNEL_SIZE = 3       # 3x3 Filter size, A SQUARE kernel.\n",
    "DECODE_KERNEL_CPLX = 64      # 32 channel/filter/kernel\n",
    "DECODE_KERNEL_SMPL = 32      # 32 channel/filter/kernel\n",
    "DECODE_KERNEL_ONE = 1        # Single channel/filter/kernel\n",
    "DENSE_LAYER_UNITS = SPATIAL_ALPHA * SPATIAL_OMEGA * DECODE_NEURONS_COMPLEX\n",
    "ACTIVATION_FUNCTION = 'relu'   # 'relu', 'sigmoid', and 'tanh'\n",
    "ACTIVATION_DECODE = 'sigmoid'  # 'relu', 'sigmoid', and 'tanh'\n",
    "STRIDE = 2                   # Nos of Pixels to move by\n",
    "PADS = 'same'                # Padding: Output size === Input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeX-PAiYOw1B"
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "# Latent Space Dimensionality\n",
    "latent_dim = LATENT_DIM_SIZE\n",
    "\n",
    "# Decoder In\n",
    "decoder_inputs = tf.keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# Decoder Layers\n",
    "x = layers.Dense(DENSE_LAYER_UNITS,\n",
    "                 activation=ACTIVATION_FUNCTION)(decoder_inputs)\n",
    "\n",
    "# Reshape Layers Reshape to prepare for convolutional transpose layers\n",
    "x = layers.Reshape((SPATIAL_ALPHA,\n",
    "                    SPATIAL_OMEGA,\n",
    "                    DECODE_NEURONS_COMPLEX))(x)\n",
    "\n",
    "# Conv2DTranspose Layers: Upsample and extract features: More features\n",
    "x = layers.Conv2DTranspose(DECODE_KERNEL_CPLX,\n",
    "                           DECODE_KERNEL_SIZE,\n",
    "                           activation=ACTIVATION_FUNCTION,\n",
    "                           strides=STRIDE,\n",
    "                           padding=PADS)(x)\n",
    "\n",
    "# Conv2DTranspose Layers: Upsample / extract features: More highlighted features\n",
    "x = layers.Conv2DTranspose(DECODE_KERNEL_SMPL,\n",
    "                           DECODE_KERNEL_SIZE,\n",
    "                           activation=ACTIVATION_FUNCTION,\n",
    "                           strides=STRIDE,\n",
    "                           padding=PADS)(x)\n",
    "\n",
    "# Output Layer: Generate the final reconstructed image: sigmoid.\n",
    "decoder_outputs = layers.Conv2DTranspose(DECODE_KERNEL_ONE,\n",
    "                                         DECODE_KERNEL_SIZE,\n",
    "                                         activation=ACTIVATION_DECODE,\n",
    "                                         padding=PADS)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1vjuUNoakrD"
   },
   "source": [
    "> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjShiZR3gGLq"
   },
   "source": [
    "## 6. <ins>VAE Model</ins>\n",
    "\n",
    "> This code is creating a Variational Autoencoder (VAE) by\n",
    "> - combining the encoder and decoder models.\n",
    "\n",
    "1.   Encoder Model\n",
    "     - compresses the input images into a simplified latent representation\n",
    "2.   Decoder Model\n",
    "     - reconstructs the images from this latent space.\n",
    "4.   VAE Outputs - Final\n",
    "     - Encoding:\n",
    "       - input encoder_inputs is passed through the encoder model\n",
    "       - compresses the input into a lower-dimensional latent representation.\n",
    "     - Extracting Latent Representation\n",
    "       - outputs a list containing the mean, log variance, and the sampled latent representation\n",
    "       - extracts the sampled latent representation using index ENCODER_INDEX\n",
    "           - assumed: the correct index for the latent representation\n",
    "     - Decoding: reconstructs the original input from this compressed representation\n",
    "     - Output: the final output of the VAE model.    \n",
    "3.   VAE Model\n",
    "     - final VAE model takes input images.\n",
    "     - this processes them through the encoder to get the latent representation.\n",
    "     - use the decoder to output the reconstructed images.\n",
    "\n",
    "\n",
    "- Constants for cleaner code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c31PWZqonbe6"
   },
   "outputs": [],
   "source": [
    "# VAE Model Constants\n",
    "\n",
    "ENCODER_NAME = 'encoder'\n",
    "DECODER_NAME = 'decoder'\n",
    "VAE_NAME = 'vae'\n",
    "ENCODER_INDEX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyZAhtjVO4eJ"
   },
   "outputs": [],
   "source": [
    "# VAE Model\n",
    "\n",
    "# Encoding: Compressed Images\n",
    "encoder = tf.keras.Model(encoder_inputs,\n",
    "                         [z_mean,\n",
    "                          z_log_var,\n",
    "                          z],\n",
    "                         name=ENCODER_NAME)\n",
    "\n",
    "# Decoding: Reconstructs Images\n",
    "decoder = tf.keras.Model(decoder_inputs,\n",
    "                         decoder_outputs,\n",
    "                         name=DECODER_NAME )\n",
    "\n",
    "# Final VAE decodings by encoding inputs\n",
    "outputs = decoder(encoder(encoder_inputs)[ENCODER_INDEX])\n",
    "\n",
    "# VAE Model\n",
    "vae = tf.keras.Model(encoder_inputs,\n",
    "                     outputs,\n",
    "                     name=VAE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmjO9F5BanI8"
   },
   "source": [
    "> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZPZR3HngJp8"
   },
   "source": [
    "## 7. <ins>VAE Loss</ins>\n",
    "\n",
    "> This code block is defining and adding a custom loss function to the Variational Autoencoder (VAE).\n",
    "\n",
    "1. Custom Loss Function (VAE)\n",
    "  - combines the reconstruction loss,\n",
    "    - measures how well the VAE reconstructs the input images, and\n",
    "  - KL divergence loss,\n",
    "    - ensures the latent space is well-behaved and regularized.\n",
    "  \n",
    "\n",
    "2. The combined loss helps the VAE learn to generate realistic and diverse outputs.\n",
    "3. Finally, the VAE is compiled with\n",
    "the Adam optimizer for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPU9XlgVreKi"
   },
   "outputs": [],
   "source": [
    "DATA_WIDTH_LOSS = DATA_WIDTH\n",
    "DATA_HEIGHT_LOSS = DATA_HEIGHT\n",
    "KL_BELL_CURVE_CONSTANT\n",
    "KL_AXIS = -1\n",
    "KL_LOSS_SCALE = -0.5\n",
    "WEIGHT_BIAS_OPTIMISER = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VInaOSS7O8P8"
   },
   "outputs": [],
   "source": [
    "# VAE Loss\n",
    "\n",
    "## Reconstruction Loss\n",
    "# Binary Cross Entropy\n",
    "reconstruction_loss = tf.keras.losses.binary_crossentropy(\n",
    "                           tf.keras.backend.flatten(encoder_inputs),\n",
    "                           tf.keras.backend.flatten(outputs)\n",
    "                       )\n",
    "\n",
    "# Scalling the Loss by input image dimensions\n",
    "reconstruction_loss *= DATA_WIDTH_LOSS * DATA_HEIGHT_LOSS\n",
    "\n",
    "## KL divergence loss\n",
    "# KL Block 1:  Element-wise Calculation: Calculating KL divergence/element\n",
    "kl_loss = KL_BELL_CURVE_CONSTANT\n",
    "kl_loss += z_log_var\n",
    "kl_loss -= tf.keras.backend.square(z_mean)\n",
    "kl_loss -= tf.keras.backend.exp(z_log_var)\n",
    "\n",
    "# KL Block 2: Aggregation & Scaling: aggregates these individual losses & scales them\n",
    "kl_loss = tf.keras.backend.sum(kl_loss,\n",
    "                               axis=KL_AXIS)\n",
    "kl_loss *= KL_LOSS_SCALE\n",
    "\n",
    "## VAE Loss\n",
    "# Ave the sum of the reconstruction loss & KL divergence loss.\n",
    "vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "## Optimisation: Adam: Weights & Biases: Update models parameters, on calc loss.\n",
    "vae.compile(optimizer=WEIGHT_BIAS_OPTIMISER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CECTyT3kapES"
   },
   "source": [
    "> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4SeQikcgM8i"
   },
   "source": [
    "# 8. <ins>Train the model</ins>\n",
    "\n",
    "> Tells the VAE to train on the x_train dataset for 30 epochs, processing 128 images at a time, and evaluating its progress on the x_test dataset after each epoch (i.e. None for no correspondng validating data values).\n",
    "\n",
    "**Function**\n",
    "1. `vae.fit( ... )`\n",
    "   - Starts the training process for the VAE model.\n",
    "   - Takes the training data\n",
    "   - Adjusts the model's parameters (weights and biases) iteratively,\n",
    "   - Evaluates its performance.\n",
    "**Parameters**\n",
    "2. `x_train`: raining dataset containing the input images.\n",
    "3. `epochs=30`: number of times the VAE will iterate over the entire training dataset.\n",
    "4. `batch_size=128`: This determines the number of images the VAE processes at once.\n",
    "5. `validation_data=(x_test, None)`: This provides a separate dataset.\n",
    "   - `x_test`, likely the MNIST test set\n",
    "   - `TARGET_VALUES=None`: Are no corresponding target values for the validation data. VAE is a generative model and doesn't require explicit targets for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fWkm04swr8l"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "TRAINING_EPOCHS = 30                      # Nos of Training Iterations\n",
    "TRAINING_BACTCH_SIZE = 128                # No of Images per batch\n",
    "TARGET_VALUES = None                      # No corresponding target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAWZdwlkPA4C"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "vae.fit(\n",
    "        x_train,                          # Training Input Dataset\n",
    "        epochs=TRAINING_EPOCHS,           # Nos of Training Iterations\n",
    "        batch_size=TRAINING_BACTCH_SIZE,  # No of Images per batch\n",
    "        validation_data=(                 # Validation data:\n",
    "                x_test,                     # Separate Test Set\n",
    "                TARGET_VALUES))             # No corresponding target values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mo4pbwjarAe"
   },
   "source": [
    "> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zho9LQCegQe0"
   },
   "source": [
    "# 9. <ins>Display the generated images - function</ins>\n",
    "\n",
    "1.  Display a `n*n` 2D manifold of digits\n",
    "2.  Transform linearly spaced coordinates\n",
    "    - Linearly spaced coordinates on the unit squares\n",
    "        - Transformed through the inverse CDF (ppf) of the Gaussian\n",
    "        - To produce values of the latent variables `z`,\n",
    "        - Since the prior of the latent space is Gaussian\n",
    "3.  Generates Image Grid\n",
    "    - Evenly spacing of a grid\n",
    "    - Places the reshaped images in the grid\n",
    "4.  Initialise the Figure creation\n",
    "5.  Set the pixel boundaries\n",
    "6.  Initialise the plots features\n",
    "7.  Display the decode images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIec6TSA0H3H"
   },
   "outputs": [],
   "source": [
    "# Display Constants\n",
    "GRID_SIZE = 30\n",
    "FIGURE_SIZE = 15\n",
    "MINIST_STD_SIZE = 28\n",
    "SCALE_FACTOR = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roILXD_sBCi4"
   },
   "source": [
    "### 9.1 *Auxillary Functions*\n",
    "\n",
    "**Functions**\n",
    "*   Create Figure: Initialing for plotting\n",
    "*   Calculate Pixel Ranges\n",
    "*   PLot axes: Samples, Axes, Ticks, Labels,\n",
    "*   Display Decoded Images:\n",
    "    - `plt.imshow(figure, cmap=cmap)`: displays the image data stored in the figure array.\n",
    "    - `cmap='Greys_r`' sets a colormap to `\"Greys_r`\": a reversed grayscale colormap,\n",
    "    - `plt.show()`: renders the plot, making the image grid visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-p3Bplew9Qwa"
   },
   "outputs": [],
   "source": [
    "def create_figure(figsize):\n",
    "    # Inits figure for plotting\n",
    "    return plt.figure(figsize=(figsize, figsize))\n",
    "\n",
    "def calculate_pixel_range(digit_size, n, centering=2):\n",
    "    # Start\n",
    "    start_range = digit_size // centering\n",
    "    # End\n",
    "    end_range = n * digit_size + start_range\n",
    "    # Pixel Size\n",
    "    return np.arange(start_range,\n",
    "                     end_range,\n",
    "                     digit_size)\n",
    "\n",
    "def plot_axes(grid_x,\n",
    "             grid_y,\n",
    "             pixel_range,\n",
    "             label_x=\"z[0]\",\n",
    "             label_y=\"z[1]\",\n",
    "             round_factor=1):\n",
    "    # Samples\n",
    "    sample_range_x = np.round(grid_x, round_factor)\n",
    "    sample_range_y = np.round(grid_y, round_factor)\n",
    "\n",
    "    # Axes Ticks, Labels\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    # Labels\n",
    "    plt.xlabel(label_x)\n",
    "    plt.ylabel(label_y)\n",
    "\n",
    "def display_decoded_images(figure, cmap='Greys_r'):\n",
    "    plt.imshow(figure, cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOBRONeEC23t"
   },
   "source": [
    "### 9.2 *Display Generated Images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_UzwteuPF6O"
   },
   "outputs": [],
   "source": [
    "# Display generated images\n",
    "def plot_latent_space(decoder,\n",
    "                      n=GRID_SIZE,\n",
    "                      figsize=FIGURE_SIZE):\n",
    "\n",
    "    # Display a n*n 2D manifold of digits\n",
    "    digit_size = MINIST_STD_SIZE\n",
    "    scale = SCALE_FACTOR\n",
    "    figure = np.zeros((digit_size * n,\n",
    "                       digit_size * n))\n",
    "\n",
    "\n",
    "    def generate_image_grid(grid_scale, grid_size):\n",
    "\n",
    "        # Private Constants\n",
    "        _REVERSE_SLICE = -1\n",
    "        _FIRST_ELEMENT = 0\n",
    "        _FIG_INCREMENT = 1\n",
    "\n",
    "        # Private Labels - Original Syntax\n",
    "        ROW_INDEX = 'i'\n",
    "        Y_COORDINATE = 'yi'\n",
    "        COLUMN_INDEX = 'j'\n",
    "        X_COORDINATE = 'xi'\n",
    "        LATENT_POINT = 'z_sample'\n",
    "        DECODED_IMAGE = 'x_decoded'\n",
    "        RESHAPED_DIGIT = 'digit'\n",
    "        DIGITAL_SIZE = 'digit_size'\n",
    "\n",
    "        grid_x = np.linspace(-grid_scale, grid_scale, size)\n",
    "        grid_y = np.linspace(-grid_scale, grid_scale, size)[::_REVERSE_SLICE]\n",
    "\n",
    "        for ROW_INDEX, Y_COORDINATE in enumerate(grid_y):\n",
    "            for COLUMN_INDEX, X_COORDINATE in enumerate(grid_x):\n",
    "                # Latent Point\n",
    "                LATENT_POINT = np.array([[X_COORDINATE, Y_COORDINATE]])\n",
    "                # Decoded Image\n",
    "                DECODED_IMAGE = decoder.predict(LATENT_POINT)\n",
    "                # Reshaped Image\n",
    "                RESHAPED_DIGIT = DECODED_IMAGE[_FIRST_ELEMENT].\\\n",
    "                                    reshape(DIGITAL_SIZE, DIGITAL_SIZE)\n",
    "                # Rows & Columns\n",
    "                start_row = ROW_INDEX * DIGIT_SIZE\n",
    "                end_row = (ROW_INDEX + _FIG_INCREMENT) * DIGITAL_SIZE\n",
    "                start_col = COLUMN_INDEX * DIGITAL_SIZE\n",
    "                end_col = (COLUMN_INDEX + _FIG_INCREMENT) * DIGITAL_SIZE\n",
    "\n",
    "                # Figure's reshaped Digit\n",
    "                figure[start_row:end_row, start_col:end_col] = RESHAPED_DIGIT\n",
    "\n",
    "\n",
    "    # Call the inner function\n",
    "    generate_image_grid(scale, n)\n",
    "    # Figure Creation\n",
    "    figure = create_figure(figsize)\n",
    "    # Pixel Boundaries\n",
    "    pixel_range = calculate_pixel_range(digit_size, n)\n",
    "    # Axes\n",
    "    plot_axes(grid_x, grid_y, pixel_range)\n",
    "    # Display Images\n",
    "    display_decoded_images(figure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x4s8v74at3v"
   },
   "source": [
    "> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFOnrVypgWu6"
   },
   "source": [
    "## 10. <ins>Run the Model</ins>\n",
    "\n",
    "1. Polt Latent Space:\n",
    "   - Displays the decoded images.\n",
    "        - Model: Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8idKqGJPLVp"
   },
   "outputs": [],
   "source": [
    "# run the model\n",
    "plot_latent_space(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLJE4wNCiF4q"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "> <center> ~ # ~ </center>\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePVyeieYiLVP"
   },
   "source": [
    "## Author\n",
    "\n",
    "[![LinkedIn](https://img.shields.io/badge/Author-Charles%20J%20Fowler-0077B5?logo=gmail&logoColor=white)](mailto:ipoetdev-github-no-reply@outlook.com \"Contact CJ on GItHub email: ipoetdev-github-no-reply@outlook.com\") <sup>|</sup> [![LinkedIn](https://img.shields.io/badge/Charles%20J%20Fowler-LinkedIn-0077B5?logo=linkedin&logoColor=white)](https://ie.linkedin.com/in/charlesjfowler \"@CharlesJFowler @Linkedin.com\") <sup>|</sup> [![LinkedIn](https://img.shields.io/badge/iPoetDev-GitHub-0077B5?logo=GitHub&logoColor=white)](https://github.com/ipoetdev \"@iPoetDev @GitHub\")\n",
    "\n",
    "## ChangeLog\n",
    "\n",
    "| Date<sup>1</sup> | Version | Changed By | Change | Activity | From |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| 2024-07-16  | 0.1 | Charles J Fowler  | Source uploaded | Uploaded  | [Source Notebook]( https://colab.research.google.com/drive/1eD7pRKmhVFl0nfwzsoIy9RTtoPMVkZPW?usp=sharing \"Author: Marty Bradly\") |\n",
    "| 2024-07-26  | 0.2 | Charles J Fowler  | Draft Portfolio version | Modify  | --- |  \n",
    "<sup>1</sup>: `YYYY-MM-DD"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
